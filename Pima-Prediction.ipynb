{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MMUST IEEE AI & Robotics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![logo](nai-ieee.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Make sure you are using a version of notebook greater than v.3. If you installed Anaconda with python 3 - this is likely to be fine. The next piece of code will check if you have the right version.\n",
    "2. Follow along with vigor. If you're stuck, don't hesitate to ask\n",
    "3. Have fun ;-)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Jupyter Notebook__ allows you to write, document and run your code in one place. For data science work it presents a very convenient tool for quickly prototyping your scripts and algorithms.\n",
    "\n",
    "Also it includes so many plug-ins for example you can create presentation slides on the fly, you can export the notebook to a python script file, to HTML, to pdf, to LaTeX, etc. Its a great tool."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pima Indian Diabetes Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import some basic libraries.\n",
    "* Pandas - provided data frames\n",
    "* matplotlib.pyplot - plotting support\n",
    "\n",
    "Use Magic %matplotlib to display graphics inline instead of in a popup window.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd                 # pandas is a dataframe library\n",
    "import matplotlib.pyplot as plt      # matplotlib.pyplot plots data\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Reviewing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./data/pima-data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition of features\n",
    "From the metadata on the data source we have the following definition of the features.\n",
    "\n",
    "| Feature  | Description | Comments |\n",
    "|--------------|-------------|--------|\n",
    "| num_preg     | number of pregnancies         |\n",
    "| glucose_conc | Plasma glucose concentration a 2 hours in an oral glucose tolerance test         |\n",
    "| diastolic_bp | Diastolic blood pressure (mm Hg) |\n",
    "| thickness | Triceps skin fold thickness (mm) |\n",
    "|insulin | 2-Hour serum insulin (mu U/ml) |\n",
    "| bmi |  Body mass index (weight in kg/(height in m)^2) |\n",
    "| diab_pred |  Diabetes pedigree function |\n",
    "| Age (years) | Age (years)|\n",
    "| skin | ???? | What is this? |\n",
    "| diabetes | Class variable (1=True, 0=False) |  Why is our data boolean (True/False)? |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().values.any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlated Feature Check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function that displays correlation by color.  __Yellow__ is most correlated, __Purple__ least."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_corr(df, size=11):\n",
    "    \"\"\"\n",
    "    Function plots a graphical correlation matrix for each pair of columns in the dataframe.\n",
    "\n",
    "    Input:\n",
    "        df: pandas DataFrame\n",
    "        size: vertical and horizontal size of the plot\n",
    "\n",
    "    Displays:\n",
    "        matrix of correlation between columns.  Blue-cyan-yellow-red-darkred => less to more correlated\n",
    "                                                0 ------------------>  1\n",
    "                                                Expect a darkred line running from top left to bottom right\n",
    "    \"\"\"\n",
    "\n",
    "    corr = df.corr()    # data frame correlation function\n",
    "    fig, ax = plt.subplots(figsize=(size, size))\n",
    "    ax.matshow(corr)   # color code the rectangles by correlation value\n",
    "    plt.xticks(range(len(corr.columns)), corr.columns)  # draw x tick marks\n",
    "    plt.yticks(range(len(corr.columns)), corr.columns)  # draw y tick marks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_corr(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df['skin']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_corr(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correlations look good.  There appear to be no coorelated columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mold Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect data types to see if there are any issues. Data should be numeric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change diabetes from boolean to integer, True=1, False=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_map = {True: 1, False: 0}\n",
    "df['diabetes'] = df['diabetes'].map(diabetes_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check class distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rare events are hard to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_obs = len(df)\n",
    "num_true = len(df.loc[df['diabetes']==1])\n",
    "num_false = len(df.loc[df['diabetes']==0])\n",
    "\n",
    "print(\"Number of True cases: {0} ({1:2.2f}%)\".format(num_true, (num_true/num_obs)*100))\n",
    "print(\"Number of False cases: {0} ({1:2.2f}%)\".format(num_false, (num_false/num_obs)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good distribution of true and false cases.  No special work needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "70% for training, 30% for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "feature_col_names = ['num_preg', 'glucose_conc', 'diastolic_bp', 'thickness', 'insulin', 'bmi', 'diab_pred', 'age']\n",
    "predicted_class_names = ['diabetes']\n",
    "\n",
    "X = df[feature_col_names].values     # predictor feature columns (8 X m)\n",
    "y = df[predicted_class_names].values # predicted class (1=true, 0=false) column (1 X m)\n",
    "split_test_size = 0.30\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=split_test_size, random_state=42) \n",
    "                            # test_size = 0.3 is 30%, 42 is the answer to everything"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check to ensure we have the the desired 70% train, 30% test split of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"{0:0.2f}% in training set\".format((len(X_train)/len(df.index)) * 100))\n",
    "print(\"{0:0.2f}% in test set\".format((len(X_test)/len(df.index)) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Verifying predicted value was split correctly__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Original True  : {0} ({1:0.2f}%)\".format(len(df.loc[df['diabetes'] == 1]), (len(df.loc[df['diabetes'] == 1])/len(df.index)) * 100.0))\n",
    "print(\"Original False : {0} ({1:0.2f}%)\".format(len(df.loc[df['diabetes'] == 0]), (len(df.loc[df['diabetes'] == 0])/len(df.index)) * 100.0))\n",
    "print(\"\")\n",
    "print(\"Training True  : {0} ({1:0.2f}%)\".format(len(y_train[y_train[:] == 1]), (len(y_train[y_train[:] == 1])/len(y_train) * 100.0)))\n",
    "print(\"Training False : {0} ({1:0.2f}%)\".format(len(y_train[y_train[:] == 0]), (len(y_train[y_train[:] == 0])/len(y_train) * 100.0)))\n",
    "print(\"\")\n",
    "print(\"Test True      : {0} ({1:0.2f}%)\".format(len(y_test[y_test[:] == 1]), (len(y_test[y_test[:] == 1])/len(y_test) * 100.0)))\n",
    "print(\"Test False     : {0} ({1:0.2f}%)\".format(len(y_test[y_test[:] == 0]), (len(y_test[y_test[:] == 0])/len(y_test) * 100.0)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post-split Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Hidden Missing Values__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are these 0 values possible?\n",
    "\n",
    "How many rows have have unexpected 0 values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"# rows in dataframe {0}\".format(len(df)))\n",
    "print(\"# rows missing glucose_conc: {0}\".format(len(df.loc[df['glucose_conc'] == 0])))\n",
    "print(\"# rows missing diastolic_bp: {0}\".format(len(df.loc[df['diastolic_bp'] == 0])))\n",
    "print(\"# rows missing thickness: {0}\".format(len(df.loc[df['thickness'] == 0])))\n",
    "print(\"# rows missing insulin: {0}\".format(len(df.loc[df['insulin'] == 0])))\n",
    "print(\"# rows missing bmi: {0}\".format(len(df.loc[df['bmi'] == 0])))\n",
    "print(\"# rows missing diab_pred: {0}\".format(len(df.loc[df['diab_pred'] == 0])))\n",
    "print(\"# rows missing age: {0}\".format(len(df.loc[df['age'] == 0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Impute with the mean__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "#Impute with mean all 0 readings\n",
    "fill_0 = Imputer(missing_values=0, strategy=\"mean\", axis=0)\n",
    "\n",
    "X_train = fill_0.fit_transform(X_train)\n",
    "X_test = fill_0.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Initial Algorithm - Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# create Gaussian Naive Bayes model object and train it with the data\n",
    "nb_model = GaussianNB()\n",
    "\n",
    "nb_model.fit(X_train, y_train.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance on Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict values using the training data\n",
    "nb_predict_train = nb_model.predict(X_train)\n",
    "\n",
    "# import the performance metrics library\n",
    "from sklearn import metrics\n",
    "\n",
    "# Accuracy\n",
    "print(\"Accuracy: {0:.4f}\".format(metrics.accuracy_score(y_train, nb_predict_train)))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance on Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict values using the testing data\n",
    "nb_predict_test = nb_model.predict(X_test)\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "# training metrics\n",
    "print(\"Accuracy: {0:.4f}\".format(metrics.accuracy_score(y_test, nb_predict_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Metrics__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Confusion Matrix\")\n",
    "# Note the use of labels for set 1=True to upper left and 0=False to lower right\n",
    "print(\"{0}\".format(metrics.confusion_matrix(y_test, nb_predict_test, labels=[1, 0])))\n",
    "print(\"\")\n",
    "\n",
    "print(\"Classification Report\")\n",
    "print(metrics.classification_report(y_test, nb_predict_test, labels=[1,0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf_model = RandomForestClassifier(random_state=42)      # Create random forest object\n",
    "rf_model.fit(X_train, y_train.ravel()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_predict_train = rf_model.predict(X_train)\n",
    "# training metrics\n",
    "print(\"Accuracy: {0:.4f}\".format(metrics.accuracy_score(y_train, rf_predict_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_predict_test = rf_model.predict(X_test)\n",
    "\n",
    "# training metrics\n",
    "print(\"Accuracy: {0:.4f}\".format(metrics.accuracy_score(y_test, rf_predict_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.confusion_matrix(y_test, rf_predict_test, labels=[1, 0]) )\n",
    "print(\"\")\n",
    "print(\"Classification Report\")\n",
    "print(metrics.classification_report(y_test, rf_predict_test, labels=[1,0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr_model =LogisticRegression(C=0.7, random_state=42)\n",
    "lr_model.fit(X_train, y_train.ravel())\n",
    "lr_predict_test = lr_model.predict(X_test)\n",
    "\n",
    "# training metrics\n",
    "print(\"Accuracy: {0:.4f}\".format(metrics.accuracy_score(y_test, lr_predict_test)))\n",
    "print(metrics.confusion_matrix(y_test, lr_predict_test, labels=[1, 0]) )\n",
    "print(\"\")\n",
    "print(\"Classification Report\")\n",
    "print(metrics.classification_report(y_test, lr_predict_test, labels=[1,0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting regularization parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_start = 0.1\n",
    "C_end = 5\n",
    "C_inc = 0.1\n",
    "\n",
    "C_values, recall_scores = [], []\n",
    "\n",
    "C_val = C_start\n",
    "best_recall_score = 0\n",
    "while (C_val < C_end):\n",
    "    C_values.append(C_val)\n",
    "    lr_model_loop = LogisticRegression(C=C_val, random_state=42)\n",
    "    lr_model_loop.fit(X_train, y_train.ravel())\n",
    "    lr_predict_loop_test = lr_model_loop.predict(X_test)\n",
    "    recall_score = metrics.recall_score(y_test, lr_predict_loop_test)\n",
    "    recall_scores.append(recall_score)\n",
    "    if (recall_score > best_recall_score):\n",
    "        best_recall_score = recall_score\n",
    "        best_lr_predict_test = lr_predict_loop_test\n",
    "        \n",
    "    C_val = C_val + C_inc\n",
    "\n",
    "best_score_C_val = C_values[recall_scores.index(best_recall_score)]\n",
    "print(\"1st max value of {0:.3f} occured at C={1:.3f}\".format(best_recall_score, best_score_C_val))\n",
    "\n",
    "%matplotlib inline \n",
    "plt.plot(C_values, recall_scores, \"-\")\n",
    "plt.xlabel(\"C value\")\n",
    "plt.ylabel(\"recall score\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logisitic regression with class_weight='balanced'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_start = 0.1\n",
    "C_end = 5\n",
    "C_inc = 0.1\n",
    "\n",
    "C_values, recall_scores = [], []\n",
    "\n",
    "C_val = C_start\n",
    "best_recall_score = 0\n",
    "while (C_val < C_end):\n",
    "    C_values.append(C_val)\n",
    "    lr_model_loop = LogisticRegression(C=C_val, class_weight=\"balanced\", random_state=42)\n",
    "    lr_model_loop.fit(X_train, y_train.ravel())\n",
    "    lr_predict_loop_test = lr_model_loop.predict(X_test)\n",
    "    recall_score = metrics.recall_score(y_test, lr_predict_loop_test)\n",
    "    recall_scores.append(recall_score)\n",
    "    if (recall_score > best_recall_score):\n",
    "        best_recall_score = recall_score\n",
    "        best_lr_predict_test = lr_predict_loop_test\n",
    "        \n",
    "    C_val = C_val + C_inc\n",
    "\n",
    "best_score_C_val = C_values[recall_scores.index(best_recall_score)]\n",
    "print(\"1st max value of {0:.3f} occured at C={1:.3f}\".format(best_recall_score, best_score_C_val))\n",
    "\n",
    "%matplotlib inline \n",
    "plt.plot(C_values, recall_scores, \"-\")\n",
    "plt.xlabel(\"C value\")\n",
    "plt.ylabel(\"recall score\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr_model =LogisticRegression( class_weight=\"balanced\", C=best_score_C_val, random_state=42)\n",
    "lr_model.fit(X_train, y_train.ravel())\n",
    "lr_predict_test = lr_model.predict(X_test)\n",
    "\n",
    "# training metrics\n",
    "print(\"Accuracy: {0:.4f}\".format(metrics.accuracy_score(y_test, lr_predict_test)))\n",
    "print(metrics.confusion_matrix(y_test, lr_predict_test, labels=[1, 0]) )\n",
    "print(\"\")\n",
    "print(\"Classification Report\")\n",
    "print(metrics.classification_report(y_test, lr_predict_test, labels=[1,0]))\n",
    "print(metrics.recall_score(y_test, lr_predict_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LogisticRegressionCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "lr_cv_model = LogisticRegressionCV(n_jobs=-1, random_state=42, Cs=3, cv=10, refit=False, class_weight=\"balanced\")  # set number of jobs to -1 which uses all cores to parallelize\n",
    "lr_cv_model.fit(X_train, y_train.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict on Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_cv_predict_test = lr_cv_model.predict(X_test)\n",
    "\n",
    "# training metrics\n",
    "print(\"Accuracy: {0:.4f}\".format(metrics.accuracy_score(y_test, lr_cv_predict_test)))\n",
    "print(metrics.confusion_matrix(y_test, lr_cv_predict_test, labels=[1, 0]) )\n",
    "print(\"\")\n",
    "print(\"Classification Report\")\n",
    "print(metrics.classification_report(y_test, lr_cv_predict_test, labels=[1,0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Happy Coding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"float:left;margin:0 10px 10px 0\" markdown=\"1\">\n",
    "    ![sign](sign.png)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
