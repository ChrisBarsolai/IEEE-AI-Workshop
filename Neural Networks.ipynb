{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MMUST IEEE AI & Robotics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![logo](nai-ieee.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Make sure you are using a version of notebook greater than v.3. If you installed Anaconda with python 3 - this is likely to be fine. The next piece of code will check if you have the right version.\n",
    "2. Follow along with vigor. If you're stuck, don't hesitate to ask\n",
    "3. Have fun ;-)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Jupyter Notebook__ allows you to write, document and run your code in one place. For data science work it presents a very convenient tool for quickly prototyping your scripts and algorithms.\n",
    "\n",
    "Also it includes so many plug-ins for example you can create presentation slides on the fly, you can export the notebook to a python script file, to HTML, to pdf, to LaTeX, etc. Its a great tool."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks\n",
    "\n",
    "Neural networks are a powerful machine learning framework used to learn complex input-output mappings from examples. Examples of successful applications of neural networks include:\n",
    "1. Classification of handwritten digits\n",
    "2. Speech recognition\n",
    "\n",
    "Neural networks can be viewed as a series of nonlinear transformations applied to the input variables where the nature of the transformation is learned from the training data. There are several neural network architectures but we will focus on a feedforward architecture where information flows in one direction from input to output and there is no feedback from the output back to the input.\n",
    "\n",
    "## The Multilayer Perceptron\n",
    "\n",
    "The multilayer perceptron (MLP) is a feed forward neural network. The figure below shows an MLP with a single hidden layer (from http://deeplearning.net/tutorial/mlp.html).\n",
    "\n",
    "![MLP](mlp.png)\n",
    "\n",
    "The input to a given layer is obtained from the output of the previous layer. The output of a given layer is obtained by applying an activation function to a weighted linear combination of the inputs. Mathematically let $x_1,\\ldots,x_D$ be the inputs to a given layer. The output of the $j$th hidden layer is given by\n",
    "\\begin{eqnarray*}\n",
    "z_j=h(\\sum_{i=1}^Dw_{ji}x_i+w_{j0})\n",
    "\\end{eqnarray*}\n",
    "where:\n",
    "1. $h(.)$ is a nonlinear activation function\n",
    "2. $w_{ji}$ is the weight from input node (neuron) $i$ to output node $j$\n",
    "3. $w_{j0}$ is known as the bias of neuron $j$\n",
    "\n",
    "Similarly, the output $y_k$ of the $k$th output neuron is obtained by applying an activation function to a weighted linear combination of the inputs from the hidden layer. This output is a function of the weights $\\mathbf{w}$ and the inputs $\\mathbf{x}$ and we write $y_k(\\mathbf{x},\\mathbf{w})$. We can collect all the outputs into a vector $\\mathbf{y}(\\mathbf{x},\\mathbf{w})$\n",
    "\n",
    "\n",
    "### Activation Functions\n",
    "There are a number of activation functions used depending on the nature of the data and target variables. These include:\n",
    "1. The sigmoid function \n",
    "\\begin{eqnarray*}\n",
    "\\sigma(a)=\\frac{1}{1+\\exp(-a)}\n",
    "\\end{eqnarray*}\n",
    "2. The Tanh function \n",
    "\\begin{eqnarray*}\n",
    "\\tanh(a)=\\frac{\\exp(a)-\\exp(-a)}{\\exp(a)+\\exp(-a)}\n",
    "\\end{eqnarray*}\n",
    "3. The rectified linear unit (ReLU)\n",
    "\\begin{eqnarray*}\n",
    "f(a)=max\\{o,a\\}\n",
    "\\end{eqnarray*}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write three functions to plot the three activation functions.\n",
    "\n",
    "\n",
    "#The functions you write should take in a vector of points and return \n",
    "#the activation function evaluated at those points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Sigmoid Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting Sigmoid Function\n",
    "# Your Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Tahn Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting Tahn Function\n",
    "# Your Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Rectified Linear Unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting ReLU\n",
    "# Your Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Learning\n",
    "\n",
    "The aim of training the neural network is to learn an input-output mapping from examples. We aim to learn a set of weights and biases to obtain the appropriate mapping. Given $N$ training examples $\\mathbf{x}_n$ and the correspinding target output vectors $\\mathbf{t}_n$, we aim to learn weights and biases to minimize the error\n",
    "\\begin{eqnarray*}\n",
    "E(\\mathbf{w})=\\frac{1}{2}\\sum_{n=1}^N||\\mathbf{y}(\\mathbf{x}_n,\\mathbf{w})-\\mathbf{t}_n)||^2\n",
    "\\end{eqnarray*}\n",
    "\n",
    "This learning is often achieved by gradient descent where the weights at one time step $\\tau$ are modified in the direction of negative gradient according to \n",
    "\\begin{eqnarray*}\n",
    "\\mathbf{w}^{(\\tau+1)}=\\mathbf{w}^{(\\tau)}-\\eta\\nabla E(\\mathbf{w}^{(\\tau)})\n",
    "\\end{eqnarray*}\n",
    "where $\\eta>0$ is the learning rate. In practice, for the MLP the gradient of the error function is found by backpropagation.\n",
    "\n",
    "### Example\n",
    "We will consider the example of learning to classify hand written digits. We will use the MNIST training set which consists of 70,000 examples and the implementation of the MLP on scikit learn http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the data\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.datasets import fetch_mldata\n",
    "mnist = fetch_mldata('MNIST original',data_home='scikit_learn_data/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us explore the data by plotting a few examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The flattened images of the digits are 784 dimensional arrays\n",
    "mnist.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The targets are stored in mnist.target\n",
    "mnist.target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot a random digit and show the label\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "i=np.random.randint(0, mnist.data.shape[0]-1)\n",
    "plt.imshow(mnist.data[i].reshape(28,28),cmap='Greys_r')\n",
    "plt.xticks([]);\n",
    "plt.yticks([]);\n",
    "print(int(mnist.target[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write code to divide the data into training, validation and test sets\n",
    "#X_train,X_val,X_test=\n",
    "#y_test,y_val,y_test=\n",
    "\n",
    "#Normalise the data by dividing the pixel values by 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "mnist.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X = mnist.data/255\n",
    "y = mnist.target\n",
    "\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=.4)# 60% for training\n",
    "X_val,X_test,y_val,y_test=train_test_split(X_test,y_test,test_size=.5) # 20% validation and 20 % test\n",
    "\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use the data to select an appropriate number of hidden layer neurons\n",
    "#for an MLP with a single hidden layer the function\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the number of hidden neurons per layer as a tuple\n",
    "HL=(100,)\n",
    "\n",
    "\n",
    "#create the classifier\n",
    "mlp = MLPClassifier(hidden_layer_sizes=HL,activation='relu' ,max_iter=20, alpha=1e-4,\n",
    "                        solver='sgd', verbose=10, tol=1e-4, random_state=1,\n",
    "                        learning_rate_init=.1)\n",
    "\n",
    "mlp.fit(X_train, y_train)\n",
    "print(\"Training set score: %.2f\" % mlp.score(X_train, y_train))\n",
    "print(\"Validation set score: %f\" % mlp.score(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modify the above code to search through MLPs with 50,100,150,200,500 and 1000 neurons\n",
    "#in the hidden layer to find the best\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with the best network, learn a model and use it to find the accuracy \n",
    "#on the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us view some of the digits in the test set and corresponding classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use mlp.predict to see how your classifier labels some test digits\n",
    "#mlp.predict(TestDigit)\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "i=np.random.randint(0, X_test.shape[0]-1)\n",
    "plt.imshow(X_test[i].reshape(28,28),cmap='Greys_r')\n",
    "plt.xticks([]);\n",
    "plt.yticks([]);\n",
    "\n",
    "print(\"Inference Output:\")\n",
    "print(mlp.predict(X_test[i].reshape(1,-1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Happy Coding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"float:left;margin:0 10px 10px 0\" markdown=\"1\">\n",
    "    ![sign](sign.png)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
